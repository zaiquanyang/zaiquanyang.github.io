---
layout: post
title: 弱监督定位WSOL
tags: 弱监督学习
---


## 🦖 Adversarial Complementary Learning for WSOL CVPR-2018


### 🦖 动机
 
分类权重激活的区域往往是一些具有显著区分度的区域，而不显著区域往往被忽略，为了能够提取出尽可能多不显著的区域，作者设计一个对抗互补性学习方法。

对抗互补性学习：使用两个分支 $$A$$ 和 $$B$$ 预测 CAM, 使用其中一个分支的预测接过去 erasing 掉另一个分支，从而鼓励该分支能够关注更多不显著的区域，最后二者的预测结果综合起来作为最后的定位结果。



<div align=center><img src="http://tva1.sinaimg.cn/large/007d2DYjly1h2nam1w70oj30nt08q0wc.jpg" width="300"></div>

### 🦖 内容

模型结构如下：

<div align=center><img src="http://tva1.sinaimg.cn/large/007d2DYjly1h2nak0uobgj312v0khh0e.jpg" width="500"></div>


----


## 🦖 DANet: Divergent Activation for Weakly Supervised Object Localization ICCV-2019


---


## 🦖 Rethinking the Route Towards Weakly Supervised Object Localization CVPR_2020

----



## 🦖 Rethinking Class Activation Mapping for Weakly Supervised Object Localization ECCV_2020

### 🦖 动机分析

作者 rethink 了 CAM 应用在弱监督定位中存在的几个问题：

<div align=center><img src="http://tva1.sinaimg.cn/large/007d2DYjly1h2nya0oo5uj30zh0dm147.jpg" width="600"></div>

- 不同的通道激活的区域面积有较大区别，GAP 操作容易偏向于赋予那些激活面积小的通道更大的权重，从而使得最后的激活区域只有重叠较多的那部分较小的区域。

- 区分度不高或者说不是那么显著的区域激活值往往较低，使其在最后平均后的激活结果中显得更加不突出。


- 不同的通道之间的激活区域往往会有较多重叠部分，在加权平均后的结果中，不重叠的部分会被削弱， 而重叠  部分会被凸显出来，但是其区域面积比较有限。

感觉作者所指出的这些问题都在指向 分类任务中会用得到全局平均池化层 GAP 和 CAM 生成过程中的权重系数加权这些操作。

#### 🦖 主要工作

- 针对第一个问题：作者分析产生原因是不管每个通道激活区域大小是多少都会除以 $$H \times W$$ 造成的，因此提出了新的池化方式，不再直接除以 $$H \times W$$，而是除以大于某个激活阈值的区域面积，也就是文中的 Thresholded Average Pooling.

$$
p_{c}^{\mathrm{tap}}=\frac{\sum_{(h, w)} \mathbb{\mathbb { 1 }}\left(\mathbf{F}_{c}(h, w)>\tau_{t a p}\right) \mathbf{F}_{c}(h, w)}{\sum_{(h, w)} \mathbb{1}\left(\mathbf{F}_{c}(h, w)>\tau_{t a p}\right)}
$$

这样不管激活区域面积的通道也会被赋予较大权重。

- 针对第二个问题：

看完这篇文章对 ECCV 的文章印象更差了，不是很难的一个任务，却没有讲清楚自己的实验发现和方法，有空再好好看下作者表达的什么意思。

----

## 🦖 Strengthen Learning Tolerance for WSOL CVPR-2021

### 🦖 动机分析

当前的WSOL还存在两个方面的问题：
- part domination：只有 discriminative 的区域才能被激活
- 定位的鲁棒性，对于不同外观形状变化的物体的定位结果敏感。

作者设计的结构分为分类和回归两个训练阶段，提出的idea:
- 允许分类器出现一定的语义差错。
- 仿照半监督学习中强-弱增强技巧使回归器更加鲁棒。

### 🦖 主要工作

作者提出的这个SLT-Net 包括 localizer, regressor, and classifier, 其中localizer, regressor一起训练负责定位，classifier 负责分类，单独训练一个网络。作者这是将分类和定位拆分开了。

<div align=center><img src="http://tva1.sinaimg.cn/large/007d2DYjly1h2oknu2zvcj31890g5qju.jpg" width="600"></div>

作者怎么增强容忍性学习呢？

- 容忍语义分类错误  Learning Tolerance to Semantic Mistake， 以缓解 part domination 问题. 
作者在 localizer 中添加一个语义容忍模块，就是当只要能分类到前 $$K$$ 个类就近似认为分类正确，降低损失。
作者认为这样不仅能够缓解原先那种分类任务导致只有显著区域的问题，还能够保持一定的语义区分性，避免背景区域被激活。

$$
\mathcal{L}_{\mathrm{SMT}}(\mathbf{x}, \mathbf{y})=\left\{\begin{array}{ll}
\alpha \cdot \mathcal{L}_{\mathrm{CE}}(\mathbf{x}, \mathbf{y}), & \text { if } x_{k} \in \text { top- } K(\mathrm{x}) \\
\mathcal{L}_{\mathrm{CE}}(\mathbf{x}, \mathbf{y}), & \text { otherwise }
\end{array}\right.
$$

需要注意的是: 由于认为只要预测的前 $$K$$ 个类别含有正确类别就认为分类正确了，因此在计算CAM时也应该计算这 $$K$$ 个 类对应的CAM之后进行综合得出最终的定位结果。


-  Learning Tolerance to Visual Stimulus

作者起得名字 Visual Stimulus 不太常见的词汇，其实现就是半监督学习中教师模型输入弱增强，学生模型输入强增强，以提高学生网络的鲁棒性，衡量教师和学生网络的一致性损失使用的是 MSE 损失。

----


## 🦖 Shallow Feature Matters for WSOL CVPR-2021

<div align=center><img src="http://tva1.sinaimg.cn/large/007d2DYjly1h2n3tkezk5j30zz0cfwnx.jpg" width="600"></div>
<!-- 
<div align=center><img src="http://tva1.sinaimg.cn/large/007d2DYjly1h2n3tkezk5j30zz0cfwnx.jpg" width="600"></div> -->

## 🦖 Foreground Activation Maps for WSOL ICCV-2021


<div align=center><img src="http://tva1.sinaimg.cn/large/007d2DYjly1h2n9qqc235j30i20by41a.jpg" width="350"></div>

<div align=center><img src="http://tva1.sinaimg.cn/large/007d2DYjly1h2n9py0djoj319w0jc7m0.jpg" width="600"></div>

----

## 🦖 Bridging the Gap between Classification and Localization  for WSOD CVPR-2022

----


## 🦖 Background Activation Suppression for WSOL CVPR-2022


<div align=center><img src="http://tva1.sinaimg.cn/large/007d2DYjly1h2n9tb2l9tj31aw0irtvj.jpg" width="600"></div>






























----


## 🦖 Shallow Feature Matters for Weakly Supervised Object Localization CVPR-2021