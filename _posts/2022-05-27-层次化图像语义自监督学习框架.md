---
# layout: post
title: A General Framework for Hierarchical Image Representation Learning
tags: 自监督学习 层次化语义表征学习
---
*一篇投在NeurIPS_2022的工作*
[HIRL: A General Framework forHierarchical Image Representation Learning](https://arxiv.org/pdf/2205.13159.pdf)

[Code刚开源](https://github.com/hirl-team/HIRL)


### 🦖 动机

当前自监督学习通过学习图片之间的语义相似度在视觉任务上取得了不错的效果，但是自监督学习尚未表明能获取层级语义信息，层级语义信息在自然界中是常常存在的，因此本工作提取了一种层级语义的自监督学习框架，且表明层级语义信息有助于提升当前自监督学习性能。

本文解决了两方面问题：
- 层级语义信息是否有助于当前的自监督学习模型如 MoCO 等
- 如何在自监督学习框架下实现多层级的图像语义信息的表征学习

### 🦖 内容

首先先了解下 **Hierarchical Prototypes** 



#### 🦖 Hierarchical prototypes

`Hierarchical prototypes`是由多阶段的`K-means`计算得到的，第一阶段得到最细粒度的聚类，然后在前一阶段的基础上进行聚类，得到更粗粒度的语义聚类。作者引的是[Hcsc: Hierarchical contrastive selective coding]() 这个工作，有兴趣可以阅读。

<div align=center><img src="http://tva1.sinaimg.cn/large/007d2DYjly1h2n10bg7ssj30hg0amn2i.jpg" width="400"></div>

层级聚类输出的是包含多层级语义信息的聚类 Prototypes, $C=\left\{\left\{c_{i}^{l}\right\}_{i=1}^{M_{l}}\right\}_{l=1}^{L}$ , $L$ 表示不同语义层级， $M_{l}$ 表示第 $l$ 个层级的语义 Prototypes 个数。

#### 🦖 Semantic Path Discrimination

如果说之前的自监督学习是建模 $p\left(z^{0} \mid x\right)$ ， 那么现在要编码多层级语义， 就是建模 $p\left(z^{0}, \cdots, z^{L} \mid x\right)$ 。

作者给出的方案是 **Semantic Path Discrimination**

首先利用现有的 `SSL` 方法提取 `Vanilla image representations`， $$\tilde{Z}=\left\{\tilde{z}_{1}, \tilde{z}_{2}, \cdots, \tilde{z}_{N}\right\}$$ ， $N$ 表示图片数目。这些表征可以看作是最细粒度语义的特征 $z^0$, 其对应的语义层级空间记为  $$V_0$$ 。
为了得到更多粗粒度层级的特征，作者使用 `MLP` 将 $$V_0$$ 映射到 $$V_l$$ 层级，没有看代码，应该是多个`MLP` 并联或者串联得到多层级的特征 $$\left\{z^{l}\right\}_{l=1}^{L}$$ 。

那么如何建模多层级语义呢？如下图，首先获取每个层级的 `Prototypes`, 这些 `Prototypes` 构成了一个树结构，表明了不同层级聚类之间的从属关系。

<div align=center><img src="http://tva1.sinaimg.cn/large/007d2DYjly1h2n0kegphrj30vd0c6jzk.jpg" width="600"></div>

对于最细粒度也是树最底层的 `Persian Cat`来说，其属于 `Cat` 这个更粗的语义类别， 再往上遍历，其属于 `Mammal` 这个粗的语义类别，那么 $\textbf{Persian Cat} \Rightarrow Cat \Rightarrow Mammal$ 就构成了属于输入样本的 `Positive Path`， 其它任何路径都是 `Negative Path`, 每个路径都包含了 $L$ 个层级的 `Prototypes`。在自监督学习框架下，定义正样本 $$\mathbb{P}_{+}=\left\{c_{+}^{l}\right\}_{l=1}^{L}$$ ， 负样本是 $$\mathbb{P}_{-}=\left\{c_{-}^{l}\right\}_{l=1}^{L}$$ 。

对比学习损失为最大化输入的样本层级特征 $$\left\{z^{l}\right\}_{l=1}^{L}$$ 和正样本/正语义路径  $$\mathbb{P}_{+}=\left\{c_{+}^{l}\right\}_{l=1}^{L}$$ 之间的相似度，最小化与负样本/负语义路径  $$\mathbb{P}_{-}=\left\{c_{-}^{l}\right\}_{l=1}^{L}$$ 的相似度：


$$
s\left(\left\{z^{l}\right\}_{l = 1}^{L}, \mathbb{P}\right) = \prod_{l = 1}^{L} \frac{1+\cos \left(z^{l}, c^{l}\right)}{2}
$$

损失计算公式为：

$$
\mathcal{L}_{\mathrm{SPD}}(\theta)=-\mathbb{E}_{x \sim p_{d}}\left[\log \left(s\left(\left\{z^{l}\right\}_{l=1}^{L}, \mathbb{P}_{+}\right)\right)+\frac{1}{N_{\text {neg }}} \sum_{k=1}^{N_{\text {neg }}} \log \left(1-s\left(\left\{z^{l}\right\}_{l=1}^{L}, \mathbb{P}_{-}^{k}\right)\right)\right]
$$

### 🦖 实现细节

预训练包括两个阶段：
- 第一阶段就是普通的 SSL 方法，目的是先学习一个比较不错的细粒度语义特征吧。

- 第二阶段：每个 epoch 开始根据当前的模型提取特征并执行层级聚类，损失除了 $$\mathcal{L}_{\mathrm{SSL}}$$ 的损失外，还有本文提出的层级语义编码损失 $$\mathcal{L}_{\mathrm{SPD}}$$ .


$\mathcal{L}_{\mathrm{SSL}}$


更多的细节看原文，感觉如果不是专门做自监督学习的话，很难把这类的模型调好。

#### 🦖 实验

-  在KNN Evaluation, Linear Classification and Fine-tuning 任务评价
  <div align=center><img src="http://tva1.sinaimg.cn/large/007d2DYjly1h2n2rzhsi4j313t0e1ar4.jpg" width="800"></div>

   效果提升貌似还凑合吧

- 在检测、分割等下游任务上以及在聚类指标的评测结果：
  <div align=center><img src="http://tva1.sinaimg.cn/large/007d2DYjly1h2n2u09er4j30oe0edn6y.jpg" width="450"></div>
在聚类上的效果还算比较明显一些。
  <div align=center><img src="http://tva1.sinaimg.cn/large/007d2DYjly1h2n308hs9aj30fg0fhn5y.jpg" width="350"></div>

<!-- <table><tr>
<td><img src=http://tva1.sinaimg.cn/large/007d2DYjly1h2n2u09er4j30oe0edn6y.jpg border=0></td>
<td><img src=http://tva1.sinaimg.cn/large/007d2DYjly1h2n308hs9aj30fg0fhn5y.jpg border=0></td>
</tr></table> -->

