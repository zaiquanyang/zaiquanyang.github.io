---
title:  hubness问题介绍
tags: 论文阅读笔记
---


# 背景

最近阅读一些零次学习的相关论文，枢纽点问题`hubess`是经常出现的一个概念，对于这个概念的理解较少，只知道是会影响到零次学习的模型性能，也是 `KNN`(`k nearest neighbors`)会出现的一个现象，后来阅读了论文 **Hubs in Space: Popular Nearest Neighbors in High-Dimensional Data** 有了更深一步的了解。

[论文地址](https://www.jmlr.org/papers/volume11/radovanovic10a/radovanovic10a.pdf)

# 简述


## 基本概念

在介绍本文内容之前需要了解一些相关概念。

**偏态分布**

偏态分布(skew distribution)是与正态分布相对，分布曲线左右不对称的数据次数分布，是连续随机变量概率分布的一种。可以通过峰度和偏度的计算，衡量偏态的程度。可分为正偏态和负偏态，前者曲线右侧偏长，带有一个较长的尾巴，左侧则偏短，后者曲线在左侧有一个较长的尾巴，右侧偏短。
偏离程度可以用一个偏离系数表示，偏离稀疏大于0则表示正偏态，小于0则为负偏态，等于0则是正态分布。


$\Chi ^{2}$**分布**

卡方分布（chi-square distribution）是概率论与统计学中常用的一种概率分布。$k$ 个独立的标准正态分布变量的平方和服从自由度为$k$的卡方分布.
*数学定义*
若随机变量$ Z_1, \cdots, Z_k$是互相独立，且符合标准正态分布的随机变量，则他们的平方和
$$
X=\sum_{i=1}^{k}{Z_{i}}^{2}$$
服从自由度为 $k$ 的卡方分布，记作 $X \sim \Chi^{2}(k)$.

卡方分布的概率密度函数为：

$$
f_k(x)=\frac{\frac{1}{2}^{\frac{k}{2}}}{\Gamma({\frac{k}{2}})}x^{\frac{k}{2}-1}e^{-\frac{x}{2}}
$$

这里的 $\Gamma(\cdot)$是伽马函数，是阶乘函数在实数与复数上的扩展，如果 $n$ 为正整数，则
$\Gamma(n)=(n-1)!$, 对于实数部分为正数的复数 $z$, 伽马函数定义为：
$$
\Gamma(z)=\int_{0}^{\infin} \frac{t^{z-1}}{e^{t}}\mathbf{d}t$$

## 维度灾难

维度灾难是指随着维度的增加，数据的分布会出现一些异于低维空间的现象，这些现象给当前各种各样的机器学习模型带来了不少的挑战与困难。比较常见的维度灾难现象是`数据稀疏性`，`距离浓缩`(distance concentration)。数据稀疏性比较好理解，对于100个数据点，如果放在一维空间内的10个单位距离内，那么每个单位距离内很可能会包含数据点，但是如果对于二维空间内，就有一个100个单位，每个单位包含数据点的可能性就比较低，对于更高维空间，则每个单位内包含的数据点的概率就会极低。距离浓缩的意义为高维空间内的大部分点都远离其中心，距离的远近实际上已经变得很难区分，数学形式表示为：

$$
\lim _{d \rightarrow \infin} \frac{dist_{max}-dist_{min}}{dist_{min}} \approx 0
$$

在这种情形下，低维空间中常用的欧式距离等度量方式已经不太可行，对于像`KNN`这样的机器学习模型有着较大的影响。

本文探索研究了一种新的维度灾难现象即枢纽点问题(hubness)。具体而言就是：随着数据的维度的增加，会出现一些顺枢纽点，这些枢纽点相比于其它数据点会频繁作为数据点的`K`个最近的邻居出现，打一个比方就是，在一个类似于数据点的人类社会体系中，一些人的人脉关系比较强，和其他大多数人都有比较近的交往。

数学形式化表示：假设 $\mathcal{D} \subset \mathbb{R}^{d} $ 是一个 $d$ 维的数据点集合，用 $N_{k}(\mathbf{x})$ 表示数据点 $\mathbf{x}$ 在一定的距离计算规则下作为其他数据点 $k$ 近邻出现的次数。 

在一般的条件情形下，随着数据维度的升高， $N_{k}$ 的分布会更加趋向于正偏态分布。